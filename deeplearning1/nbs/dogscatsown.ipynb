{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 7003 on context None\n",
      "Mapped name None to device cuda: Tesla K80 (0000:00:1E.0)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# from __future__ import division,print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utils; \n",
    "#from urllib import reload\n",
    "#reload(utils)\n",
    "from utils import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data_drive/fast.ai_oren/courses/deeplearning1/nbs\n"
     ]
    }
   ],
   "source": [
    "if 'BASE_DIR' not in locals():\n",
    "    BASE_DIR = os.getcwd()\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_BASE_DIR = os.path.join(BASE_DIR, 'data/dogscatskaggle')\n",
    "\n",
    "%mkdir -p $DATA_BASE_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $DATA_BASE_DIR\n",
    "os.system(\"kg download -c dogs-vs-cats-redux-kernels-edition\")\n",
    "os.system(\"unzip -q train.zip\")\n",
    "os.system(\"unzip -q test.zip\")\n",
    "%cd $BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = os.path.join(DATA_BASE_DIR, \"train\")\n",
    "filenames = os.listdir(train_path)\n",
    "cats_path = os.path.join(train_path, \"cats\")\n",
    "dogs_path = os.path.join(train_path, \"dogs\")\n",
    "if not os.path.exists(cats_path):\n",
    "    os.makedirs(cats_path)\n",
    "if not os.path.exists(dogs_path):\n",
    "    os.makedirs(dogs_path)\n",
    "        \n",
    "for filename in filenames:\n",
    "    if \"cat\" in filename:\n",
    "        os.rename(os.path.join(train_path, filename), os.path.join(cats_path, filename[4:]))\n",
    "    elif \"dog\" in filename:\n",
    "        os.rename(os.path.join(train_path, filename), os.path.join(dogs_path, filename[4:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare train, validation, sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/ubuntu/data_drive')\n",
    "sys.path.insert(0, '/home/ubuntu/data_drive/deeputils')\n",
    "import deeputils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data_drive/fast.ai_oren/courses/deeplearning1/nbs/data/dogscatskaggle\n",
      "number of items to move:  0\n",
      "src_dir: /home/ubuntu/data_drive/fast.ai_oren/courses/deeplearning1/nbs/data/dogscatskaggle/train/cats number of items to copy: 0\n",
      "src_dir: /home/ubuntu/data_drive/fast.ai_oren/courses/deeplearning1/nbs/data/dogscatskaggle/train/dogs number of items to copy: 0\n",
      "src_dir: /home/ubuntu/data_drive/fast.ai_oren/courses/deeplearning1/nbs/data/dogscatskaggle/test number of items to copy: 0\n",
      "src_dir: /home/ubuntu/data_drive/fast.ai_oren/courses/deeplearning1/nbs/data/dogscatskaggle/valid/cats number of items to copy: 0\n",
      "src_dir: /home/ubuntu/data_drive/fast.ai_oren/courses/deeplearning1/nbs/data/dogscatskaggle/valid/dogs number of items to copy: 0\n"
     ]
    }
   ],
   "source": [
    "print(DATA_BASE_DIR)\n",
    "deeputils.file.split_train(DATA_BASE_DIR)\n",
    "deeputils.file.gen_sample(DATA_BASE_DIR, split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-fetch images and save as one large array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/dogscatskaggle/\"\n",
    "# path = \"data/dogscatskaggle/sample/\"\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_train = deeptuils.imageutils.get_data(path+ 'train')\n",
    "# data_valid = deeptuils.imageutils.get_data(path+ 'valid')\n",
    "# print(data_train.shape)\n",
    "# print(data_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "352\n",
      "40\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "batches_train = image.ImageDataGenerator().flow_from_directory(path+'train', shuffle=False, batch_size=batch_size, \n",
    "                                         class_mode='categorical', target_size=(224, 224))\n",
    "batches_valid = image.ImageDataGenerator().flow_from_directory(path+'valid', shuffle=False, batch_size=batch_size, \n",
    "                                         class_mode='categorical', target_size=(224, 224))\n",
    "batches_test = image.ImageDataGenerator().flow_from_directory(path+'test', shuffle=False, batch_size=batch_size, \n",
    "                                         class_mode='categorical', target_size=(224, 224))\n",
    "steps_train = math.ceil(batches_train.samples / batch_size)\n",
    "steps_valid = math.ceil(batches_valid.samples / batch_size)\n",
    "steps_test = math.ceil(batches_test.samples / batch_size)\n",
    "print(steps_train)\n",
    "print(steps_valid)\n",
    "print(steps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_train = batches_train.classes\n",
    "classes_valid = batches_valid.classes\n",
    "classes_test = batches_test.classes\n",
    "labels_train = deeputils.general.onehot(classes_train)\n",
    "labels_valid = deeputils.general.onehot(classes_valid)\n",
    "labels_test = deeputils.general.onehot(classes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pass datasets through pre-trained convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.models import Model\n",
    "\n",
    "input_tensor = Input(shape=(3, 224,224))\n",
    "model_suffix = \"_vgg19\"\n",
    "conv_model = VGG19(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "for layer in conv_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 512, 7, 7)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 512, 7, 7)\n",
      "(2500, 512, 7, 7)\n",
      "(12500, 512, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "features_train = conv_model.predict_generator(batches_train, steps_train)\n",
    "print(features_train.shape)\n",
    "features_valid = conv_model.predict_generator(batches_valid, steps_valid)\n",
    "print(features_valid.shape)\n",
    "features_test = conv_model.predict_generator(batches_test, steps_test)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deeputils.file.save_array(model_path + 'train_convlayers_features'+model_suffix+'.bc', features_train)\n",
    "deeputils.file.save_array(model_path + 'valid_convlayers_features'+model_suffix+'.bc', features_valid)\n",
    "deeputils.file.save_array(model_path + 'test_convlayers_features'+model_suffix+'.bc', features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train fc layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 512, 7, 7)\n",
      "(2500, 512, 7, 7)\n",
      "(12500, 512, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "features_train = deeputils.file.load_array(model_path + 'train_convlayers_features'+model_suffix+'.bc')\n",
    "features_valid = deeputils.file.load_array(model_path + 'valid_convlayers_features'+model_suffix+'.bc')\n",
    "features_test = deeputils.file.load_array(model_path + 'test_convlayers_features'+model_suffix+'.bc')\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "\n",
    "#def proc_wgts(layer): return [o/2 for o in layer.get_weights()]\n",
    "\n",
    "# opt = Adam()\n",
    "def get_fc_model(dropout, lr):\n",
    "    model = Sequential([\n",
    "        MaxPooling2D(input_shape=conv_model.layers[-1].output_shape[1:]),\n",
    "#         Flatten(input_shape=conv_model.layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "        Dropout(dropout),\n",
    "        Dense(4096, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "        Dropout(dropout),\n",
    "        Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "#     opt = RMSprop(lr=lr, rho=0.7)\n",
    "    opt = Adam(lr=lr)\n",
    "    \n",
    "#     for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(l2.get_weights())\n",
    "\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 6s - loss: 1.5969 - acc: 0.8936 - val_loss: 1.3853 - val_acc: 0.9116\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 6s - loss: 0.8706 - acc: 0.9428 - val_loss: 0.5226 - val_acc: 0.9652\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 6s - loss: 0.7581 - acc: 0.9511 - val_loss: 0.5899 - val_acc: 0.9612\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 6s - loss: 0.7440 - acc: 0.9515 - val_loss: 0.6083 - val_acc: 0.9612\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 6s - loss: 0.7489 - acc: 0.9522 - val_loss: 0.7554 - val_acc: 0.9520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcbb5a4fc18>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model05 = get_fc_model(0.5, 0.0001)\n",
    "fc_model05.fit(features_train, labels_train, epochs = 5, batch_size=batch_size,\n",
    "             validation_data = (features_valid, labels_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 119,586,818\n",
      "Trainable params: 119,570,434\n",
      "Non-trainable params: 16,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fc_model05.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.6876 - acc: 0.9331 - val_loss: 0.3793 - val_acc: 0.9660\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.3371 - acc: 0.9683 - val_loss: 0.3065 - val_acc: 0.9696\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.2614 - acc: 0.9759 - val_loss: 0.4492 - val_acc: 0.9616\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.2340 - acc: 0.9776 - val_loss: 0.2974 - val_acc: 0.9724\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.1883 - acc: 0.9825 - val_loss: 0.3155 - val_acc: 0.9740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36a258e898>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model00 = get_fc_model(0.2, 0.00001)\n",
    "fc_model00.fit(features_train, labels_train, epochs = 5, batch_size=batch_size,\n",
    "             validation_data = (features_valid, labels_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.1631 - acc: 0.9847 - val_loss: 0.3686 - val_acc: 0.9692\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.1472 - acc: 0.9859 - val_loss: 0.3099 - val_acc: 0.9756\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.1132 - acc: 0.9898 - val_loss: 0.2937 - val_acc: 0.9740\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.1123 - acc: 0.9895 - val_loss: 0.3735 - val_acc: 0.9660\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.1004 - acc: 0.9907 - val_loss: 0.2930 - val_acc: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36a20b7080>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model00.optimizer.lr = 0.000001\n",
    "fc_model00.fit(features_train, labels_train, epochs = 5, batch_size=batch_size,\n",
    "             validation_data = (features_valid, labels_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.0903 - acc: 0.9917 - val_loss: 0.3425 - val_acc: 0.9692\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.0769 - acc: 0.9933 - val_loss: 0.3252 - val_acc: 0.9684\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.0885 - acc: 0.9924 - val_loss: 0.3216 - val_acc: 0.9724\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.0675 - acc: 0.9939 - val_loss: 0.3386 - val_acc: 0.9724\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 7s - loss: 0.0640 - acc: 0.9941 - val_loss: 0.3307 - val_acc: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36a1c62e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model00.optimizer.lr = 0.0000001\n",
    "fc_model00.fit(features_train, labels_train, epochs = 5, batch_size=batch_size,\n",
    "             validation_data = (features_valid, labels_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 6s - loss: 1.3644 - acc: 0.9152 - val_loss: 0.9735 - val_acc: 0.9396\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 6s - loss: 1.2372 - acc: 0.9232 - val_loss: 0.9800 - val_acc: 0.9392\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 6s - loss: 1.1953 - acc: 0.9258 - val_loss: 0.9285 - val_acc: 0.9424\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 6s - loss: 1.4199 - acc: 0.9119 - val_loss: 1.3232 - val_acc: 0.9176\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 6s - loss: 1.3137 - acc: 0.9185 - val_loss: 0.9408 - val_acc: 0.9416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad9ccdef28>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model00.optimizer.lr = 0.0001\n",
    "fc_model00.fit(features_train, labels_train, epochs = 5, batch_size=batch_size,\n",
    "             validation_data = (features_valid, labels_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fc_model00.save_weights(path+'no_dropout.h5')\n",
    "# fc_model00.save(path+'no_dropout_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=conv_model.input, outputs=fc_model00(conv_model.output))\n",
    "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.save_weights(model_path+'no_dropout.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(model_path+\"no_dropout_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.topology.InputLayer'>\n"
     ]
    }
   ],
   "source": [
    "for layer in conv_model.layers: \n",
    "    layer.trainable = False\n",
    "print(type(conv_model.layers[0]))\n",
    "model = Model(inputs=conv_model.input, outputs=fc_model00(conv_model.output))\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, \n",
    "                               height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
    "batches_aug_train = gen.flow_from_directory(path+'train', shuffle=True, batch_size=batch_size, \n",
    "                                         class_mode='categorical', target_size=(224, 224))\n",
    "batches_aug_valid = gen.flow_from_directory(path+'valid', shuffle=True, batch_size=batch_size, \n",
    "                                         class_mode='categorical', target_size=(224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "352/352 [==============================] - 516s - loss: 1.5751 - acc: 0.8983 - val_loss: 1.2071 - val_acc: 0.9216\n",
      "Epoch 2/3\n",
      "352/352 [==============================] - 516s - loss: 0.9987 - acc: 0.9364 - val_loss: 0.7595 - val_acc: 0.9512\n",
      "Epoch 3/3\n",
      "352/352 [==============================] - 516s - loss: 1.0292 - acc: 0.9348 - val_loss: 2.5446 - val_acc: 0.8412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fadb8bbcef0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches_aug_train, steps_per_epoch=steps_train, epochs=3, \n",
    "                              validation_data=batches_aug_valid, validation_steps=steps_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "351/352 [============================>.] - ETA: 1s - loss: 1.1082 - acc: 0.9303"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-e23538707d2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(batches_aug_train, steps_per_epoch=steps_train, epochs=3, \n\u001b[0;32m----> 2\u001b[0;31m                               validation_data=batches_aug_valid, validation_steps=steps_valid)\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2062\u001b[0m                                 \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                                 use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   2065\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2174\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2176\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(batches_aug_train, steps_per_epoch=steps_train, epochs=3, \n",
    "                              validation_data=batches_aug_valid, validation_steps=steps_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.save_weights(path+'augmented.h5')\n",
    "conv_model.save(path+'augmented_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open(model_path + 'no_dropout_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "submit_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "submit_model.load_weights(model_path + \"no_dropout.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit_model = fc_model00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions = submit_model.predict_generator(batches_test, steps_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = fc_model00.predict(features_test, steps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 2)\n",
      "[[  1.2579e-32   1.0000e+00]\n",
      " [  1.0000e+00   1.7184e-41]\n",
      " [  3.1085e-34   1.0000e+00]\n",
      " [  0.0000e+00   1.0000e+00]\n",
      " [  1.0000e+00   0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unknown/2212.jpg', 'unknown/11615.jpg', 'unknown/9910.jpg', 'unknown/6117.jpg', 'unknown/541.jpg']\n"
     ]
    }
   ],
   "source": [
    "filenames = batches_test.filenames\n",
    "print(filenames[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.0000e+00   1.7184e-41   1.0000e+00   1.0000e+00   0.0000e+00]\n"
     ]
    }
   ],
   "source": [
    "isdog = predictions[:,1]\n",
    "print(isdog[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96  0.04  0.96  0.96  0.04]\n"
     ]
    }
   ],
   "source": [
    "margin = 0.04\n",
    "isdog = isdog.clip(min=margin, max=1-margin)\n",
    "print(isdog[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2212 11615  9910 ...,     6  7220  4813]\n"
     ]
    }
   ],
   "source": [
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.2120e+03,   9.6000e-01],\n",
       "       [  1.1615e+04,   4.0000e-02],\n",
       "       [  9.9100e+03,   9.6000e-01],\n",
       "       [  6.1170e+03,   9.6000e-01],\n",
       "       [  5.4100e+02,   4.0000e-02]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_file_name = 'submission1.csv'\n",
    "np.savetxt(model_path + submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/dogscatskaggle/models/submission1.csv' target='_blank'>data/dogscatskaggle/models/submission1.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data_drive/fast.ai_oren/courses/deeplearning1/nbs/data/dogscatskaggle/models/submission1.csv"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink(model_path + submission_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 512, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "features_test = conv_model.predict_generator(batches_test, steps_test)\n",
    "print(features_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deeputils.file.save_array(model_path + 'test_convlayers_features.bc', features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 512, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "features_test = deeputils.file.load_array(model_path + 'test_convlayers_features.bc')\n",
    "print(features_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensambling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    model = Sequential([\n",
    "#         MaxPooling2D(input_shape=conv_model.layers[-1].output_shape[1:]),\n",
    "        Flatten(input_shape=conv_model.layers[-1].output_shape[1:]),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "#     opt = RMSprop(lr=lr, rho=0.7)\n",
    "    opt = Adam(lr=0.00001)\n",
    "    \n",
    "#     for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(l2.get_weights())\n",
    "\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(features_train, labels_train, epochs = 5, batch_size=batch_size,\n",
    "             validation_data = (features_valid, labels_valid))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.1431 - acc: 0.9459 - val_loss: 0.0658 - val_acc: 0.9752\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0313 - acc: 0.9886 - val_loss: 0.0660 - val_acc: 0.9740\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0148 - acc: 0.9950 - val_loss: 0.0718 - val_acc: 0.9728\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0071 - acc: 0.9984 - val_loss: 0.0763 - val_acc: 0.9732\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0059 - acc: 0.9984 - val_loss: 0.0685 - val_acc: 0.9740\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.1436 - acc: 0.9451 - val_loss: 0.0770 - val_acc: 0.9684\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0330 - acc: 0.9872 - val_loss: 0.0730 - val_acc: 0.9724\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0148 - acc: 0.9952 - val_loss: 0.0667 - val_acc: 0.9724\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0106 - acc: 0.9968 - val_loss: 0.0695 - val_acc: 0.9744\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0053 - acc: 0.9991 - val_loss: 0.0744 - val_acc: 0.9740\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.1443 - acc: 0.9435 - val_loss: 0.0632 - val_acc: 0.9764\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0297 - acc: 0.9892 - val_loss: 0.0635 - val_acc: 0.9764\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0182 - acc: 0.9934 - val_loss: 0.0657 - val_acc: 0.9772\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0672 - val_acc: 0.9752\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0733 - val_acc: 0.9752\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.1496 - acc: 0.9436 - val_loss: 0.0722 - val_acc: 0.9728\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0347 - acc: 0.9874 - val_loss: 0.0699 - val_acc: 0.9720\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0150 - acc: 0.9954 - val_loss: 0.0705 - val_acc: 0.9708\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0821 - val_acc: 0.9712\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0752 - val_acc: 0.9744\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.1422 - acc: 0.9457 - val_loss: 0.0685 - val_acc: 0.9724\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0337 - acc: 0.9874 - val_loss: 0.0626 - val_acc: 0.9728\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0162 - acc: 0.9947 - val_loss: 0.0656 - val_acc: 0.9716\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0096 - acc: 0.9973 - val_loss: 0.0680 - val_acc: 0.9728\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0075 - acc: 0.9978 - val_loss: 0.0787 - val_acc: 0.9732\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.1445 - acc: 0.9449 - val_loss: 0.0706 - val_acc: 0.9744\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0295 - acc: 0.9895 - val_loss: 0.0665 - val_acc: 0.9760\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0149 - acc: 0.9958 - val_loss: 0.0700 - val_acc: 0.9760\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0095 - acc: 0.9975 - val_loss: 0.0678 - val_acc: 0.9784\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 23s - loss: 0.0066 - acc: 0.9986 - val_loss: 0.0716 - val_acc: 0.9780\n"
     ]
    }
   ],
   "source": [
    "# models = \"\"\n",
    "models = [fit_model() for i in range(6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2432/2500 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "evals = np.array([m.evaluate(features_valid, labels_valid) for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0736  0.9748]\n"
     ]
    }
   ],
   "source": [
    "print(evals.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = np.stack([m.predict(features_test) for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(all_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_preds = all_preds.mean(axis=0)\n",
    "# avg_preds = all_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.9839e-01   2.5321e-07   9.9969e-01   9.9999e-01   2.2903e-10]\n"
     ]
    }
   ],
   "source": [
    "isdog = avg_preds[:,1]\n",
    "print(isdog[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96  0.04  0.96  0.96  0.04]\n"
     ]
    }
   ],
   "source": [
    "margin = 0.04\n",
    "isdog = isdog.clip(min=margin, max=1-margin)\n",
    "print(isdog[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unknown/2212.jpg', 'unknown/11615.jpg', 'unknown/9910.jpg', 'unknown/6117.jpg', 'unknown/541.jpg']\n"
     ]
    }
   ],
   "source": [
    "filenames = batches_test.filenames\n",
    "print(filenames[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2212 11615  9910 ...,     6  7220  4813]\n"
     ]
    }
   ],
   "source": [
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.2120e+03,   9.6000e-01],\n",
       "       [  1.1615e+04,   4.0000e-02],\n",
       "       [  9.9100e+03,   9.6000e-01],\n",
       "       [  6.1170e+03,   9.6000e-01],\n",
       "       [  5.4100e+02,   4.0000e-02]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_file_name = 'submission1.csv'\n",
    "np.savetxt(model_path + submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
